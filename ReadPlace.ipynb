{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv-commday')",
   "metadata": {
    "interpreter": {
     "hash": "6a3bf993ace44fdb532309f1ccff84d6fffa29e794348c2676b78f7fcac4655d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Place Reader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The Idea of the project is to train a model to read news articles (first in english) and recognize only from the text where the event happened. I used the OpenAI GPT-2 pretrained model as a starting point and fine tuned it using the Reuters Dataset to teach it how to answer the PLACE question when having an article 'body' as input.  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Read a file from the reuters dataset\n",
    "I google around and found the reuters dataset, which is perfect as it has articles and a places tag, which is the information my model needs to learn how to read. I downloaded the SGM files locally and loaded one to test the loading pipeline. SGM is a XML like file with nested information under some 'headers'. BeautifulSoup can also read SGM files and 'parse' for the different headers inside the file. Now I need to look for the body and place of each article."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'body': ['Showers continued throughout the week in\\nthe Bahia cocoa zone, alleviating the drought since early\\nJanuary and improving prospects for the coming temporao,\\nalthough normal humidity levels have not been restored,\\nComissaria Smith said in its weekly review.\\n    The dry period means the temporao will be late this year.\\n    Arrivals for the week ended February 22 were 155,221 bags\\nof 60 kilos making a cumulative total for the season of 5.93\\nmln against 5.81 at the same stage last year. Again it seems\\nthat cocoa delivered earlier on consignment was included in the\\narrivals figures.\\n    Comissaria Smith said there is still some doubt as to how\\nmuch old crop cocoa is still available as harvesting has\\npractically come to an end. With total Bahia crop estimates\\naround 6.4 mln bags and sales standing at almost 6.2 mln there\\nare a few hundred thousand bags still in the hands of farmers,\\nmiddlemen, exporters and processors.\\n    There are doubts as to how much of this cocoa would be fit\\nfor export as shippers are now experiencing dificulties in\\nobtaining +Bahia superior+ certificates.\\n    In view of the lower quality over recent weeks farmers have\\nsold a good part of their cocoa held on consignment.\\n    Comissaria Smith said spot bean prices rose to 340 to 350\\ncruzados per arroba of 15 kilos.\\n    Bean shippers were reluctant to offer nearby shipment and\\nonly limited sales were booked for March shipment at 1,750 to\\n1,780 dlrs per tonne to ports to be named.\\n    New crop sales were also light and all to open ports with\\nJune/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\\nunder New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\\nper tonne FOB.\\n    Routine sales of butter were made. March/April sold at\\n4,340, 4,345 and 4,350 dlrs.\\n    April/May butter went at 2.27 times New York May, June/July\\nat 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\\n2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\\n2.27 times New York Dec, Comissaria Smith said.\\n    Destinations were the U.S., Covertible currency areas,\\nUruguay and open ports.\\n    Cake sales were registered at 785 to 995 dlrs for\\nMarch/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\\nNew York Dec for Oct/Dec.\\n    Buyers were the U.S., Argentina, Uruguay and convertible\\ncurrency areas.\\n    Liquor sales were limited with March/April selling at 2,325\\nand 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\\nYork July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\\nSept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\\nsaid.\\n    Total Bahia sales are currently estimated at 6.13 mln bags\\nagainst the 1986/87 crop and 1.06 mln bags against the 1987/88\\ncrop.\\n    Final figures for the period to February 28 are expected to\\nbe published by the Brazilian Cocoa Trade Commission after\\ncarnival which ends midday on February 27.\\n Reuter\\n\\x03'], 'place': ['el-salvador', 'usa', 'uruguay']}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "with open('./reuters-dataset/reut2-000.sgm', 'r') as f:\n",
    "    data = f.read()\n",
    "    soup = BeautifulSoup(data)\n",
    "    # each article is embedded in a <REUTERS> object\n",
    "    articles = soup.findAll('reuters')\n",
    "\n",
    "# read articles and store in dict\n",
    "art_dict = {}\n",
    "for num, article  in enumerate(articles):\n",
    "    art_dict[num] = {\n",
    "        'body' : [body.text for body in article.findAll('body')],\n",
    "        # use the find method for place, an article can have a list of places\n",
    "        'place' : [place.text for place in article.find('places')]\n",
    "        }\n",
    "# show one entry\n",
    "print(art_dict[0])\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Write text file as base dataset  \n",
    "Divide the dataset into training and test set. Afterwards write body and place as '<|startoftext|>' + '[CONTEXT]' + article body + '[PLACE]' + article place + '<|endoftext|>'. This tags highlight the order in which the model should read and later complete the information when asked for it. It should learn that after the [PLACE] indicator, you should write the place of the article.  \n",
    "All articles are concatenated together in a single string and then stored for further use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train dataset length: 850\nTest dataset length: 150\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_df = pd.DataFrame(data=(('. '.join(value.get('body')), ', '.join(value.get('place'))) for key, value in art_dict.items()), columns=('body', 'places'))\n",
    "\n",
    "def build_text_files(df, file_name):\n",
    "    with open(file_name, \"w\") as f:\n",
    "        d_text = ''\n",
    "        \n",
    "        #for item in art_d.keys():\n",
    "        context = \"<|startoftext|>\\n[CONTEXT]:\" + df['body']\n",
    "        place = \"\\n[PLACE]:\" + df['places']+ \"\\n<|endoftext|>\\n\"\n",
    "        d_text = context + place + \" \"\n",
    "        out_text = '\\n '.join(d_text.to_list())\n",
    "\n",
    "        f.write(out_text)\n",
    "\n",
    "\n",
    "train, test = train_test_split(text_df,test_size=0.15)\n",
    "\n",
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')\n",
    "\n",
    "print(\"Train dataset length: \"+str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                body  \\\n0  Showers continued throughout the week in\\nthe ...   \n1  Standard Oil Co and BP North America\\nInc said...   \n2  Texas Commerce Bancshares Inc's Texas\\nCommerc...   \n3  BankAmerica Corp is not under\\npressure to act...   \n4  The U.S. Agriculture Department\\nreported the ...   \n5  Argentine grain board figures show\\ncrop regis...   \n6  Red Lion Inns Limited Partnership\\nsaid it fil...   \n7  Moody's Investors Service Inc said it\\nlowered...   \n8  Champion Products Inc said its\\nboard of direc...   \n9  Computer Terminal Systems Inc said\\nit has com...   \n\n                      places  \n0  el-salvador, usa, uruguay  \n1                        usa  \n2                        usa  \n3                usa, brazil  \n4                        usa  \n5                  argentina  \n6                        usa  \n7                        usa  \n8                        usa  \n9                        usa  \n"
     ]
    }
   ],
   "source": [
    "# check places in the input dataset are stored properly\n",
    "print(text_df.head(10))"
   ]
  },
  {
   "source": [
    "# Train Model\n",
    "I started playing with the Hugging-Face transformers package (based on this notebook) which offers an easy to use playground to implement and fine tune the language models available with Hugging-Face. After playing for a while I decided I wanted to implement my model using native tensorflow to have a deeper look into the model. The methods ofered by hugging face can also be fine tuned and set with the parameters you wish. My thought was to use this model to train tensorflow rather than becoming an 'expert' in hugging face. Focus on one thing at a time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Define variables to load input data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "source": [
    "## Build TextDataset\n",
    "The text needs to be transformed into a tensor object (TF Dataset) that the model can take as input. Furthermore, the shape of the dataset has to be consistent, each input vector needs to have the same length, thats why the text (or the equivalent collection of tokens) is divided into blocks of the same length and then into batches that tensorflow can read in parallel.  \n",
    "This code is based on this article https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171. It was crucial to get the right parameter shape to fit the TFGPT2LMHeadModel with native tensorflow.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (192621 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_dataset(\n",
    "    file_path,\n",
    "    tokenizer,\n",
    "    block_size,\n",
    "    batch_size,\n",
    "    buffer_size\n",
    "    ):\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    # Use the imported GOT-2 tokenizer\n",
    "    text_tokenized = tokenizer.encode(text)\n",
    "    # make tensorflow dataset\n",
    "    examples = []\n",
    "\n",
    "    for i in range(0, len(text_tokenized) - block_size + 1, block_size):\n",
    "        examples.append(text_tokenized[i:i + block_size])\n",
    "    inputs, labels = [], []\n",
    "    # Tensorflow Dataset needs to have an input and output(x,y)\n",
    "    for ex in examples:\n",
    "        inputs.append(ex[:-1])\n",
    "        labels.append(ex[1:])\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "block_size =128 # articles are longer, could try a 500 block size\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_dataset = load_dataset(train_path, tokenizer=tokenizer,block_size=block_size, batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE)\n",
    "test_dataset = load_dataset(test_path, tokenizer=tokenizer,block_size=block_size, batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Got warning: 'Token indices sequence length is longer than the specified maximum sequence length for this model (192621 > 1024). Running this sequence through the model will result in indexing errors'\n",
    "Define the vocab length, or number of tokens in the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(array([[17329,   352,    13, ...,  3691, 18038,    13],\n       [  307, 17955,   523, ...,  2237,   269,   912],\n       [  379, 10068,  6082, ...,   262,  2493,   286],\n       ...,\n       [  481,  8749,   262, ...,   257,   198,  5162],\n       [ 6489, 11598,  5974, ...,   220,   220,   220],\n       [ 2756, 12897,   198, ...,   198,   265,   257]], dtype=int32), array([[  352,    13,  3459, ..., 18038,    13,   198],\n       [17955,   523,   326, ...,   269,   912,  3691],\n       [10068,  6082,   286, ...,  2493,   286,  4849],\n       ...,\n       [ 8749,   262,  6341, ...,   198,  5162,  4741],\n       [11598,  5974, 22064, ...,   220,   220,   317],\n       [12897,   198,  6738, ...,   265,   257,  1803]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the input dataset\n",
    "print(train_dataset.txt)\n",
    "print(list(train_dataset.as_numpy_iterator())[0])"
   ]
  },
  {
   "source": [
    "## Define the model parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Training docu for TFGPT2LMHeadModel: https://huggingface.co/transformers/training.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "optimizer = Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n"
   ]
  },
  {
   "source": [
    "# Save and Train the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel object at 0x7f213f538f70>\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f21f01b1be0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f21f01b1be0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "94/94 [==============================] - ETA: 0s - loss: 3.1839 - logits_loss: 3.1839 - logits_accuracy: 0.4381 - past_key_values_1_accuracy: 0.0019 - past_key_values_2_accuracy: 0.0016 - past_key_values_3_accuracy: 9.9244e-04 - past_key_values_4_accuracy: 0.0016 - past_key_values_5_accuracy: 0.0026 - past_key_values_6_accuracy: 0.0011 - past_key_values_7_accuracy: 0.0010 - past_key_values_8_accuracy: 0.0013 - past_key_values_9_accuracy: 0.0023 - past_key_values_10_accuracy: 0.0014 - past_key_values_11_accuracy: 0.0034 - past_key_values_12_accuracy: 0.0027 - past_key_values_13_accuracy: 0.0019 - past_key_values_14_accuracy: 0.0023 - past_key_values_15_accuracy: 0.0015 - past_key_values_16_accuracy: 0.0016 - past_key_values_17_accuracy: 0.0014 - past_key_values_18_accuracy: 0.0013 - past_key_values_19_accuracy: 0.0027 - past_key_values_20_accuracy: 0.0023 - past_key_values_21_accuracy: 0.0016 - past_key_values_22_accuracy: 0.0011 - past_key_values_23_accuracy: 0.0018 - past_key_values_24_accuracy: 0.0015 WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "94/94 [==============================] - 4244s 45s/step - loss: 3.1802 - logits_loss: 3.1802 - logits_accuracy: 0.4385 - past_key_values_1_accuracy: 0.0019 - past_key_values_2_accuracy: 0.0016 - past_key_values_3_accuracy: 9.9233e-04 - past_key_values_4_accuracy: 0.0016 - past_key_values_5_accuracy: 0.0026 - past_key_values_6_accuracy: 0.0011 - past_key_values_7_accuracy: 0.0010 - past_key_values_8_accuracy: 0.0013 - past_key_values_9_accuracy: 0.0023 - past_key_values_10_accuracy: 0.0014 - past_key_values_11_accuracy: 0.0034 - past_key_values_12_accuracy: 0.0027 - past_key_values_13_accuracy: 0.0019 - past_key_values_14_accuracy: 0.0023 - past_key_values_15_accuracy: 0.0015 - past_key_values_16_accuracy: 0.0016 - past_key_values_17_accuracy: 0.0014 - past_key_values_18_accuracy: 0.0013 - past_key_values_19_accuracy: 0.0027 - past_key_values_20_accuracy: 0.0023 - past_key_values_21_accuracy: 0.0016 - past_key_values_22_accuracy: 0.0011 - past_key_values_23_accuracy: 0.0018 - past_key_values_24_accuracy: 0.0015 - val_loss: 2.4811 - val_logits_loss: 2.4811 - val_logits_accuracy: 0.5252 - val_past_key_values_1_accuracy: 0.0019 - val_past_key_values_2_accuracy: 0.0015 - val_past_key_values_3_accuracy: 0.0010 - val_past_key_values_4_accuracy: 0.0015 - val_past_key_values_5_accuracy: 0.0023 - val_past_key_values_6_accuracy: 0.0011 - val_past_key_values_7_accuracy: 9.3088e-04 - val_past_key_values_8_accuracy: 0.0012 - val_past_key_values_9_accuracy: 0.0023 - val_past_key_values_10_accuracy: 0.0015 - val_past_key_values_11_accuracy: 0.0034 - val_past_key_values_12_accuracy: 0.0022 - val_past_key_values_13_accuracy: 0.0019 - val_past_key_values_14_accuracy: 0.0021 - val_past_key_values_15_accuracy: 0.0012 - val_past_key_values_16_accuracy: 0.0016 - val_past_key_values_17_accuracy: 0.0014 - val_past_key_values_18_accuracy: 0.0013 - val_past_key_values_19_accuracy: 0.0026 - val_past_key_values_20_accuracy: 0.0024 - val_past_key_values_21_accuracy: 0.0016 - val_past_key_values_22_accuracy: 0.0010 - val_past_key_values_23_accuracy: 0.0017 - val_past_key_values_24_accuracy: 0.0015\n",
      "Epoch 2/2\n",
      "94/94 [==============================] - 4315s 46s/step - loss: 2.3817 - logits_loss: 2.3817 - logits_accuracy: 0.5308 - past_key_values_1_accuracy: 0.0019 - past_key_values_2_accuracy: 0.0015 - past_key_values_3_accuracy: 8.9987e-04 - past_key_values_4_accuracy: 0.0015 - past_key_values_5_accuracy: 0.0025 - past_key_values_6_accuracy: 0.0012 - past_key_values_7_accuracy: 0.0011 - past_key_values_8_accuracy: 0.0013 - past_key_values_9_accuracy: 0.0023 - past_key_values_10_accuracy: 0.0015 - past_key_values_11_accuracy: 0.0032 - past_key_values_12_accuracy: 0.0024 - past_key_values_13_accuracy: 0.0018 - past_key_values_14_accuracy: 0.0023 - past_key_values_15_accuracy: 0.0015 - past_key_values_16_accuracy: 0.0017 - past_key_values_17_accuracy: 0.0013 - past_key_values_18_accuracy: 0.0014 - past_key_values_19_accuracy: 0.0026 - past_key_values_20_accuracy: 0.0023 - past_key_values_21_accuracy: 0.0017 - past_key_values_22_accuracy: 0.0011 - past_key_values_23_accuracy: 0.0017 - past_key_values_24_accuracy: 0.0015 - val_loss: 2.4160 - val_logits_loss: 2.4160 - val_logits_accuracy: 0.5352 - val_past_key_values_1_accuracy: 0.0020 - val_past_key_values_2_accuracy: 0.0016 - val_past_key_values_3_accuracy: 0.0011 - val_past_key_values_4_accuracy: 0.0015 - val_past_key_values_5_accuracy: 0.0030 - val_past_key_values_6_accuracy: 0.0010 - val_past_key_values_7_accuracy: 0.0011 - val_past_key_values_8_accuracy: 0.0012 - val_past_key_values_9_accuracy: 0.0024 - val_past_key_values_10_accuracy: 0.0014 - val_past_key_values_11_accuracy: 0.0032 - val_past_key_values_12_accuracy: 0.0023 - val_past_key_values_13_accuracy: 0.0015 - val_past_key_values_14_accuracy: 0.0019 - val_past_key_values_15_accuracy: 0.0013 - val_past_key_values_16_accuracy: 0.0018 - val_past_key_values_17_accuracy: 0.0013 - val_past_key_values_18_accuracy: 0.0014 - val_past_key_values_19_accuracy: 0.0026 - val_past_key_values_20_accuracy: 0.0023 - val_past_key_values_21_accuracy: 0.0017 - val_past_key_values_22_accuracy: 0.0011 - val_past_key_values_23_accuracy: 0.0017 - val_past_key_values_24_accuracy: 0.0015\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tb_callback = TensorBoard(\n",
    "        log_dir='./logs/gpt2-places',\n",
    "        histogram_freq=1,\n",
    "        write_graph=True\n",
    "    )\n",
    "\n",
    "# The trick with the loss parameter is fundamental to match the shape in the GPT-2 Model\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric]\n",
    "    )\n",
    "print(model)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1,\n",
    "    epochs=2,\n",
    "    batch_size=128,\n",
    "    callbacks=tb_callback\n",
    ")\n",
    "model.save_pretrained('./gpt2-places/')"
   ]
  },
  {
   "source": [
    "# Test the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at ./gpt2-places.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt2-places.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "locator = pipeline('text-generation', model='./gpt2-places', tokenizer='gpt2')\n"
   ]
  },
  {
   "source": [
    " Get some random news article"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"During a surge in attacks on Asian-Americans last spring, Andrew Yang — then recently off the 2020 presidential campaign trail — wrote an op-ed suggesting that 'we Asian-Americans need to embrace and show our Americanness in ways we never have before.' To many Asian-Americans, the message seemed to place yet another burden on victims, and it stung. One year later, as Mr. Yang hopes to make history as New York City’s first Asian-American mayor, some New Yorkers have not forgotten that op-ed, or their sense that Mr. Yang’s remarks during the presidential campaign — describing himself as “an Asian man who likes math,” for instance — could feed stereotypical tropes. But many Asian-Americans also see in his candidacy an opportunity for representation at the highest level of city government, an increasingly meaningful metric amid violent attacks on Asian-Americans in New York and across the nation, including the fatal shootings in the Atlanta area last week that left eight people dead, six of them women of Asian descent. 'I grew up Asian-American in New York, and I was always accustomed to a certain level of bullying, of racism, but it took a form of mockery, of invisibility, of disdain,' an emotional Mr. Yang said at a news conference in Times Square the next day. 'That has metastasized into something far darker. You can feel it on the streets of New York.'\"\n",
    "test_text = '<|startoftext|>\\n[CONTEXT]:' + test + '\\n[PLACE]:'"
   ]
  },
  {
   "source": [
    " ### Get Result for test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "<|startoftext|>\n",
      "[CONTEXT]:During a surge in attacks on Asian-Americans last spring, Andrew Yang — then recently off the 2020 presidential campaign trail — wrote an op-ed suggesting that 'we Asian-Americans need to embrace and show our Americanness in ways we never have before.' To many Asian-Americans, the message seemed to place yet another burden on victims, and it stung. One year later, as Mr. Yang hopes to make history as New York City’s first Asian-American mayor, some New Yorkers have not forgotten that op-ed, or their sense that Mr. Yang’s remarks during the presidential campaign — describing himself as “an Asian man who likes math,” for instance — could feed stereotypical tropes. But many Asian-Americans also see in his candidacy an opportunity for representation at the highest level of city government, an increasingly meaningful metric amid violent attacks on Asian-Americans in New York and across the nation, including the fatal shootings in the Atlanta area last week that left eight people dead, six of them women of Asian descent. 'I grew up Asian-American in New York, and I was always accustomed to a certain level of bullying, of racism, but it took a form of mockery, of invisibility, of disdain,' an emotional Mr. Yang said at a news conference in Times Square the next day. 'That has metastasized into something far darker. You can feel it on the streets of New York.'\n",
      "[PLACE]:usa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = locator(test_text, max_length=500)[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = \"South African Adrian Nel is one of the victims of a brutal insurgency being waged by militant Islamists in northern Mozambique. Dozens of bodies have been seen but few details have emerged about the other casualties. Mr Nel would have celebrated his 41st birthday on 1 April. But his body now lies in a mortuary in Pemba - a coastal city in Mozambique's resource-rich Cabo Delgado province. The region has become the latest front line in the global war being waged by militant Islamists. On Wednesday, hundreds of those militants stormed the town of Palma. Mr Nel was among dozens of people who were trying to flee the attack. His body was recovered by fleeing survivors - including his father and younger brother. Speaking to the BBC from her home in South Africa, Nel's mother, Meryl Knox, said her son leaves behind his French-Canadian wife and three children - a 10-year-old boy, and two girls, aged six and two. 'He was an absolutely beautiful father, and a beautiful person all round,' she said. 'There's been so many messages of comfort from people that have known him throughout the years. And he will be terribly, terribly missed.' Mr Nel was a commercial diver who had lost his job in South Africa because of the devastating impact of Covid-19. He moved to Mozambique in January to join his father and younger brother in the construction industry, building workers' accommodation camps in Palma, which has become the hub of a burgeoning gas industry following the offshore discovery of one of the largest natural gas fields in Africa. A mere three months later, he faced a cruel death, having been shot by militants who had carried out a four-day assault on the town, targeting shops, banks, a military barracks and the Amarula Hotel, where Nel, his father and younger brother had taken refuge along with other expatriates.\"\n",
    "test2_text = '<|startoftext|>\\n[CONTEXT]:' + test_2 + '\\n[PLACE]:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "<|startoftext|>\n",
      "[CONTEXT]:South African Adrian Nel is one of the victims of a brutal insurgency being waged by militant Islamists in northern Mozambique. Dozens of bodies have been seen but few details have emerged about the other casualties. Mr Nel would have celebrated his 41st birthday on 1 April. But his body now lies in a mortuary in Pemba - a coastal city in Mozambique's resource-rich Cabo Delgado province. The region has become the latest front line in the global war being waged by militant Islamists. On Wednesday, hundreds of those militants stormed the town of Palma. Mr Nel was among dozens of people who were trying to flee the attack. His body was recovered by fleeing survivors - including his father and younger brother. Speaking to the BBC from her home in South Africa, Nel's mother, Meryl Knox, said her son leaves behind his French-Canadian wife and three children - a 10-year-old boy, and two girls, aged six and two. 'He was an absolutely beautiful father, and a beautiful person all round,' she said. 'There's been so many messages of comfort from people that have known him throughout the years. And he will be terribly, terribly missed.' Mr Nel was a commercial diver who had lost his job in South Africa because of the devastating impact of Covid-19. He moved to Mozambique in January to join his father and younger brother in the construction industry, building workers' accommodation camps in Palma, which has become the hub of a burgeoning gas industry following the offshore discovery of one of the largest natural gas fields in Africa. A mere three months later, he faced a cruel death, having been shot by militants who had carried out a four-day assault on the town, targeting shops, banks, a military barracks and the Amarula Hotel, where Nel, his father and younger brother had taken refuge along with other expatriates.\n",
      "[PLACE]:south-africa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result2 = locator(test2_text, max_length=500)[0]['generated_text']\n",
    "print(result2)"
   ]
  },
  {
   "source": [
    "## First Results\n",
    "News in USA or UK get predicted easily. It has a harder time when the article is from another place"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Ideas\n",
    "- Model works well with countries. Get a dataset where more specific places (city, towns, streets) are saves as features.\n",
    "  \n",
    "cleaning Ideas\n",
    "- remove the Reuter\\n\\x03 ending of each article\n",
    "- check on the distribution of places\n",
    "- Add special tokens to tokenizer\n",
    "\n",
    "Develop Ideas\n",
    "- Plot a map of the place of article (articles of the day in a webpage)\n",
    "- Train in spanish"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}